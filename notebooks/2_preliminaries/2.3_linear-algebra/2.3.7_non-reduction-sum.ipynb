{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.3.7 Non Reduction Sum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Keeping Axes Unchanged in Reduction Operations\n",
        "\n",
        "When calculating the **sum** or **mean**, it can be useful to keep the number of axes unchanged.  \n",
        "- This is important when we want to use the **broadcast mechanism** later.  \n",
        "- To achieve this, we can specify the `keepdims=True` argument in the function, which retains the reduced axes with size 1 in the output tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "A=torch.arange(6, dtype=torch.float32).reshape(2,3)\n",
        "B=A.clone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 3.],\n",
              "         [12.]]),\n",
              " torch.Size([2, 1]))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum_A = A.sum(axis=1, keepdim=True)\n",
        "sum_A, sum_A.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalizing Rows with Broadcasting\n",
        "\n",
        "By keeping the axes after summing, we can use **broadcasting** to normalize a matrix.  \n",
        "- For example, if `sum_A` retains its two axes after summing each row, we can divide the original matrix `A` by `sum_A` using broadcasting.  \n",
        "- This creates a matrix where each row sums up to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.3333, 0.6667],\n",
              "        [0.2500, 0.3333, 0.4167]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A / sum_A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cumulative Sum of Tensor Elements\n",
        "\n",
        "To calculate the **cumulative sum** of elements in a tensor along a specific axis (e.g., `axis=0` for row-by-row), we can use the `cumsum` function.  \n",
        "- The `cumsum` function computes the cumulative sum without reducing the input tensor along any axis.  \n",
        "- The output tensor retains the same shape as the input tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 1., 2.],\n",
              "        [3., 5., 7.]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A.cumsum(axis=0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "d2l",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
