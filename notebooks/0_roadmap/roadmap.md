# ğŸ—ºï¸ D2L Ã–ÄŸrenim Yol HaritasÄ±

> **Dive into Deep Learning** kitabÄ±nÄ±n kapsamlÄ± Ã§alÄ±ÅŸma rehberi.  
> Python, C++ ve CUDA entegrasyonlarÄ± ile teoriden pratiÄŸe derin Ã¶ÄŸrenme.

---

## ğŸ“‹ Genel BakÄ±ÅŸ

Bu repo, D2L kitabÄ±nÄ±n sistematik bir Ã§alÄ±ÅŸmasÄ±nÄ± iÃ§erir. Her bÃ¶lÃ¼m iÃ§in:
- âœï¸ **Teorik notlar** (Markdown + LaTeX)
- ğŸ’» **Pratik kodlar** (Python + PyTorch)
- âš¡ **CUDA implementasyonlarÄ±** (performans karÅŸÄ±laÅŸtÄ±rmalarÄ±)
- ğŸ“Š **GÃ¶rselleÅŸtirmeler**
- ğŸ”¬ **Deneyler ve sonuÃ§lar**

---

## ğŸ“š BÃ¶lÃ¼m Durumu

| # | BÃ¶lÃ¼m | Notebook | Python | CUDA | Durum |
|---|-------|----------|--------|------|-------|
| **01** | Introduction | ğŸ”² | ğŸ”² | â– | BaÅŸlanmadÄ± |
| **02** | Preliminaries | ğŸ”² | ğŸ”² | âœ… | BaÅŸlanmadÄ± |
| | 2.1 Data Manipulation | ğŸ”² | ğŸ”² | â– | |
| | 2.2 Data Preprocessing | ğŸ”² | ğŸ”² | â– | |
| | 2.3 Linear Algebra | ğŸ”² | ğŸ”² | âœ… | matmul kernels |
| | 2.4 Calculus | ğŸ”² | ğŸ”² | â– | |
| | 2.5 Automatic Differentiation | ğŸ”² | ğŸ”² | â– | |
| | 2.6 Probability & Statistics | ğŸ”² | ğŸ”² | â– | |
| **03** | Linear Regression | ğŸ”² | âœ… | âœ… | BaÅŸlanmadÄ± |
| **04** | Linear Classification | ğŸ”² | âœ… | âœ… | BaÅŸlanmadÄ± |
| **05** | Multilayer Perceptrons | ğŸ”² | âœ… | âœ… | BaÅŸlanmadÄ± |
| **06** | Builder's Guide | ğŸ”² | ğŸ”² | â– | BaÅŸlanmadÄ± |
| **07** | Convolutional Neural Networks | ğŸ”² | âœ… | âœ… | BaÅŸlanmadÄ± |
| **08** | Modern CNNs | ğŸ”² | âœ… | âœ… | BaÅŸlanmadÄ± |
| | 8.1 AlexNet | ğŸ”² | âœ… | â– | |
| | 8.2 VGG | ğŸ”² | âœ… | â– | |
| | 8.3 NiN | ğŸ”² | âœ… | â– | |
| | 8.4 GoogLeNet | ğŸ”² | âœ… | â– | |
| | 8.5 Batch Normalization | ğŸ”² | âœ… | âœ… | |
| | 8.6 ResNet | ğŸ”² | âœ… | â– | |
| | 8.7 DenseNet | ğŸ”² | âœ… | â– | |
| **09** | Recurrent Neural Networks | ğŸ”² | âœ… | â– | BaÅŸlanmadÄ± |
| **10** | Modern RNNs | ğŸ”² | âœ… | â– | BaÅŸlanmadÄ± |
| | 10.1 LSTM | ğŸ”² | âœ… | â– | |
| | 10.2 GRU | ğŸ”² | âœ… | â– | |
| | 10.5 Seq2Seq | ğŸ”² | âœ… | â– | |
| **11** | Attention & Transformers | ğŸ”² | âœ… | âœ… | BaÅŸlanmadÄ± |
| | 11.3 Attention Mechanisms | ğŸ”² | âœ… | âœ… | scaled dot-product |
| | 11.5 Multi-Head Attention | ğŸ”² | âœ… | âœ… | |
| | 11.7 Transformer | ğŸ”² | âœ… | â– | |
| **12** | Optimization | ğŸ”² | âœ… | â– | BaÅŸlanmadÄ± |
| **13** | Computational Performance | ğŸ”² | ğŸ”² | âœ… | BaÅŸlanmadÄ± |
| **14** | Computer Vision | ğŸ”² | âœ… | âœ… | BaÅŸlanmadÄ± |
| **15** | NLP: Pretraining | ğŸ”² | ğŸ”² | â– | BaÅŸlanmadÄ± |
| **16** | NLP: Applications | ğŸ”² | ğŸ”² | â– | BaÅŸlanmadÄ± |
| **17** | Reinforcement Learning | ğŸ”² | ğŸ”² | â– | BaÅŸlanmadÄ± |
| **18** | Gaussian Processes | ğŸ”² | ğŸ”² | â– | BaÅŸlanmadÄ± |
| **19** | Hyperparameter Optimization | ğŸ”² | ğŸ”² | â– | BaÅŸlanmadÄ± |
| **20** | GANs | ğŸ”² | ğŸ”² | â– | BaÅŸlanmadÄ± |
| **21** | Recommender Systems | ğŸ”² | ğŸ”² | â– | BaÅŸlanmadÄ± |

**Durum Ä°konlarÄ±:**
- ğŸ”² BaÅŸlanmadÄ±
- ğŸ”„ Devam ediyor
- âœ… TamamlandÄ±
- â– Uygulanamaz

---

## âš¡ CUDA Ä°mplementasyonlarÄ±

### Tamamlanan Kerneller
- âœ… **Matrix Multiplication** (Naive + Tiled)
- âœ… **Activation Functions** (ReLU, Sigmoid, Tanh, Softmax)
- âœ… **Convolution 2D** (Naive)
- âœ… **Scaled Dot-Product Attention**
- âœ… **Loss Functions** (MSE, Cross-Entropy)

### Planlanan Kerneller
- ğŸ”² Batch Normalization
- ğŸ”² Layer Normalization
- ğŸ”² Dropout
- ğŸ”² Max/Avg Pooling
- ğŸ”² Embedding Lookup
- ğŸ”² LSTM Cell
- ğŸ”² Optimizers (SGD, Adam)

---

## ğŸ¯ Ã–ÄŸrenim Hedefleri

### Faz 1: Temel Kavramlar (BÃ¶lÃ¼m 1-6)
**Hedef:** Derin Ã¶ÄŸrenmenin temel taÅŸlarÄ±nÄ± anlamak
- [ ] Tensor iÅŸlemleri ve PyTorch temelleri
- [ ] Otomatik tÃ¼rev alma (autograd)
- [ ] DoÄŸrusal modeller (regresyon + sÄ±nÄ±flandÄ±rma)
- [ ] MLP ve backpropagation
- [ ] Overfitting, underfitting, regularization

### Faz 2: KonvolÃ¼syonel AÄŸlar (BÃ¶lÃ¼m 7-8)
**Hedef:** GÃ¶rÃ¼ntÃ¼ iÅŸleme iÃ§in Ã¶zelleÅŸmiÅŸ mimariler
- [ ] CNN temelleri (conv, pooling, padding)
- [ ] Modern CNN mimarileri (AlexNet â†’ ResNet â†’ DenseNet)
- [ ] Batch normalization ve diÄŸer normalizasyon teknikleri
- [ ] Transfer learning

**CUDA Proje:** Optimized 2D Convolution

### Faz 3: Diziler iÃ§in AÄŸlar (BÃ¶lÃ¼m 9-10)
**Hedef:** Zamansal verileri modellemek
- [ ] RNN temelleri
- [ ] LSTM ve GRU
- [ ] Bidirectional RNNs
- [ ] Seq2Seq modelleri

### Faz 4: Attention & Transformers (BÃ¶lÃ¼m 11)
**Hedef:** Modern NLP'nin kalbi
- [ ] Attention mekanizmasÄ±
- [ ] Multi-head attention
- [ ] Transformer mimarisi
- [ ] Vision Transformers

**CUDA Proje:** Optimized Multi-Head Attention

### Faz 5: Optimizasyon & Performans (BÃ¶lÃ¼m 12-13)
**Hedef:** EÄŸitim optimizasyonu ve hÄ±zlandÄ±rma
- [ ] Optimizasyon algoritmalarÄ± (SGD â†’ Adam)
- [ ] Learning rate scheduling
- [ ] GPU programlama ve profiling
- [ ] Model paralelizasyonu

**CUDA Proje:** Custom Optimizer Kernels

### Faz 6: Uygulama AlanlarÄ± (BÃ¶lÃ¼m 14-21)
**Hedef:** GerÃ§ek dÃ¼nya problemleri
- [ ] Computer Vision uygulamalarÄ±
- [ ] NLP uygulamalarÄ±
- [ ] Reinforcement Learning
- [ ] GANs ve Ã¼retken modeller

---

## ğŸ› ï¸ Projelendirme Stratejisi

Her bÃ¶lÃ¼m iÃ§in:

### 1. Ä°lk Okuma (1 gÃ¼n)
- D2L kitabÄ±ndan bÃ¶lÃ¼mÃ¼ oku
- Temel kavramlarÄ± not al
- AlÄ±ÅŸtÄ±rmalarÄ± incele

### 2. Notebook Implementasyonu (2-3 gÃ¼n)
- Jupyter notebook'ta teorik aÃ§Ä±klamalar yaz
- PyTorch ile implementasyon
- GÃ¶rselleÅŸtirmeler ve denemeler
- D2L kitabÄ±ndaki alÄ±ÅŸtÄ±rmalarÄ± Ã§Ã¶z

### 3. Python ModÃ¼l GeliÅŸtirme (1 gÃ¼n)
- Yeniden kullanÄ±labilir kod yazmak iÃ§in `src/d2l_custom/` altÄ±na ekle
- Test yazÄ±mÄ±
- DokÃ¼mantasyon

### 4. CUDA Ä°mplementasyonu (2-4 gÃ¼n, opsiyonel)
- Kritik operasyonlar iÃ§in CUDA kernel yaz
- Performans karÅŸÄ±laÅŸtÄ±rmasÄ± (CPU vs GPU)
- Optimizasyon deneyleri

---

## ğŸ“ˆ Ä°lerleme Takibi

### HaftalÄ±k Hedefler
- **Hafta 1-2:** BÃ¶lÃ¼m 1-3 (Temel kavramlar)
- **Hafta 3-4:** BÃ¶lÃ¼m 4-6 (Classification + MLP)
- **Hafta 5-7:** BÃ¶lÃ¼m 7-8 (CNNs)
- **Hafta 8-10:** BÃ¶lÃ¼m 9-11 (RNNs + Attention)
- **Hafta 11-12:** BÃ¶lÃ¼m 12-13 (Optimization)
- **Hafta 13+:** BÃ¶lÃ¼m 14-21 (Uygulamalar)

### Milestone'lar
- ğŸ¯ **Milestone 1:** Ä°lk end-to-end sÄ±nÄ±flandÄ±rma modeli (MNIST)
- ğŸ¯ **Milestone 2:** ResNet implementasyonu ve eÄŸitimi
- ğŸ¯ **Milestone 3:** Ã–zel CUDA kernel kÃ¼tÃ¼phanesi
- ğŸ¯ **Milestone 4:** Transformer scratch'ten implementasyon
- ğŸ¯ **Milestone 5:** BÃ¼yÃ¼k proje (CV veya NLP)

---

## ğŸ’¡ Kaynaklar

### Ana Kaynaklar
- [D2L Website](https://d2l.ai/) â€” Ä°nteraktif kitap
- [D2L PyTorch](https://d2l.ai/chapter_installation/index.html) â€” PyTorch versiyonu

### CUDA KaynaklarÄ±
- [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)
- [CUTLASS](https://github.com/NVIDIA/cutlass) â€” NVIDIA'nÄ±n CUDA templates
- [Optimizing CUDA Kernels](https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/)

### Community
- [D2L Discussion Forum](https://discuss.d2l.ai/)
- [PyTorch Forums](https://discuss.pytorch.org/)
- [CUDA subreddit](https://www.reddit.com/r/CUDA/)

---

## ğŸ“ Notlar

- Her notebook standalone Ã§alÄ±ÅŸabilir olmalÄ±
- CUDA kernelleri PyTorch fallback'e sahip olmalÄ±
- Code review iÃ§in her hafta bir checkpoint
- BÃ¼yÃ¼k modeller iÃ§in checkpoint sistemi kur

---

**Son GÃ¼ncelleme:** 2026-02-09  
**Repo Versiyonu:** 0.1.0  
**Lisans:** MIT (Educational Use)
