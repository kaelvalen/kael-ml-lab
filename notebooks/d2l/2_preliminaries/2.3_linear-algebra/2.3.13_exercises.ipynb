{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Prove that the transpose of the transpose of a matrix is the matrix itself: \\( (A^\\top)^\\top = A \\).\n",
        "\n",
        "The transpose of a matrix \\( A \\) is obtained by flipping its rows and columns. If \\( A \\) is an \\( m \\times n \\) matrix, then \\( A^\\top \\) is an \\( n \\times m \\) matrix where the element at position \\( (i, j) \\) in \\( A \\) becomes the element at position \\( (j, i) \\) in \\( A^\\top \\). Transposing \\( A^\\top \\) again flips the rows and columns back, resulting in the original matrix \\( A \\). Thus, \\( (A^\\top)^\\top = A \\).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Show that the sum and transposition commute: \\( (A + B)^\\top = A^\\top + B^\\top \\).\n",
        "\n",
        "Let \\( A \\) and \\( B \\) be \\( m \\times n \\) matrices. The transpose of \\( A + B \\) is defined as:\n",
        "\\[\n",
        "(A + B)^\\top_{ij} = (A + B)_{ji} = A_{ji} + B_{ji}.\n",
        "\\]\n",
        "On the other hand:\n",
        "\\[\n",
        "A^\\top_{ij} + B^\\top_{ij} = A_{ji} + B_{ji}.\n",
        "\\]\n",
        "Since both expressions are equal, we have \\( (A + B)^\\top = A^\\top + B^\\top \\).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Is \\( A A^\\top \\) always symmetric for any square matrix \\( A \\)? Prove using the results of the previous exercises.\n",
        "\n",
        "Yes, \\( A A^\\top \\) is always symmetric. Proof:\n",
        "\n",
        "\\[\n",
        "(A A^\\top)^\\top = (A^\\top)^\\top A^\\top = A A^\\top.\n",
        "\\]\n",
        "Here, we used the result from question 1 that \\( (A^\\top)^\\top = A \\). Since the transpose of \\( A A^\\top \\) equals \\( A A^\\top \\), it is symmetric.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. What is the output of `len(X)` for a tensor \\( X \\) of shape \\( (2, 3, 4) \\)?\n",
        "\n",
        "The `len()` function returns the size of the first axis of the tensor. For a tensor \\( X \\) of shape \\( (2, 3, 4) \\), the output of `len(X)` is **2**.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. For a tensor \\( X \\) of arbitrary shape, does `len(X)` always correspond to the length of a certain axis of \\( X \\)? What is that axis?\n",
        "\n",
        "Yes, `len(X)` always corresponds to the **size of the first axis** (axis 0) of \\( X \\).\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Run \\( A / A.\\text{sum}(axis=1) \\) and analyze the results.\n",
        "\n",
        "When you divide \\( A \\) by \\( A.\\text{sum}(axis=1) \\), each row of \\( A \\) is divided by the sum of the elements in that row. This operation normalizes each row of \\( A \\) so that the sum of the elements in each row becomes 1.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. When traveling between two points in downtown Manhattan, what is the distance you need to cover in terms of coordinates? Can you travel diagonally?\n",
        "\n",
        "In Manhattan, the distance between two points is calculated using the **Manhattan distance**, which is the sum of the absolute differences of their coordinates:\n",
        "\\[\n",
        "\\text{Manhattan Distance} = |x_2 - x_1| + |y_2 - y_1|.\n",
        "\\]\n",
        "Diagonal travel is not allowed in Manhattan distance.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Consider a tensor of shape \\( (2, 3, 4) \\). What are the shapes of the summation outputs along axes 0, 1, and 2?\n",
        "\n",
        "- Summing along axis 0: Shape \\( (3, 4) \\).\n",
        "- Summing along axis 1: Shape \\( (2, 4) \\).\n",
        "- Summing along axis 2: Shape \\( (2, 3) \\).\n",
        "\n",
        "---\n",
        "\n",
        "### 9. Feed a tensor with three or more axes to the `linalg.norm` function and observe its output. What does this function compute for tensors of arbitrary shape?\n",
        "\n",
        "The `linalg.norm` function computes the **Frobenius norm** for tensors of arbitrary shape. It calculates the square root of the sum of the squares of all elements in the tensor:\n",
        "\\[\n",
        "\\text{Frobenius Norm} = \\sqrt{\\sum_{i,j,k,\\dots} X_{i,j,k,\\dots}^2}.\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### 10. Compute the product \\( ABC \\) for large matrices \\( A \\in \\mathbb{R}^{2^{10} \\times 2^{16}}, B \\in \\mathbb{R}^{2^{16} \\times 2^5}, C \\in \\mathbb{R}^{2^5 \\times 2^{14}} \\). Is there a difference in memory footprint and speed between \\( (AB)C \\) and \\( A(BC) \\)? Why?\n",
        "\n",
        "Yes, there is a difference. Matrix multiplication is associative, but the order of operations affects computational cost:\n",
        "\n",
        "- \\( (AB)C \\): First, compute \\( AB \\), which has shape \\( 2^{10} \\times 2^5 \\). Then multiply it with \\( C \\), which has shape \\( 2^5 \\times 2^{14} \\). The total cost is:\n",
        "  \\[\n",
        "  (2^{10} \\cdot 2^{16} \\cdot 2^5) + (2^{10} \\cdot 2^5 \\cdot 2^{14}).\n",
        "  \\]\n",
        "\n",
        "- \\( A(BC) \\): First, compute \\( BC \\), which has shape \\( 2^{16} \\times 2^{14} \\). Then multiply it with \\( A \\), which has shape \\( 2^{10} \\times 2^{16} \\). The total cost is:\n",
        "  \\[\n",
        "  (2^{16} \\cdot 2^5 \\cdot 2^{14}) + (2^{10} \\cdot 2^{16} \\cdot 2^{14}).\n",
        "  \\]\n",
        "\n",
        "Since \\( 2^{16} \\cdot 2^5 \\cdot 2^{14} \\) is much larger than \\( 2^{10} \\cdot 2^{16} \\cdot 2^5 \\), computing \\( (AB)C \\) is faster and uses less memory.\n",
        "\n",
        "---\n",
        "\n",
        "### 11. Compute \\( AB \\) or \\( AC^\\top \\) for large matrices \\( A \\in \\mathbb{R}^{2^{10} \\times 2^{16}}, B \\in \\mathbb{R}^{2^{16} \\times 2^5}, C \\in \\mathbb{R}^{2^5 \\times 2^{16}} \\). Is there a difference in speed? What changes if \\( C = B^\\top \\) without cloning memory? Why?\n",
        "\n",
        "- \\( AB \\): The result has shape \\( 2^{10} \\times 2^5 \\), and the computation involves \\( 2^{10} \\cdot 2^{16} \\cdot 2^5 \\) operations.\n",
        "- \\( AC^\\top \\): The result has shape \\( 2^{10} \\times 2^{16} \\), and the computation involves \\( 2^{10} \\cdot 2^{16} \\cdot 2^5 \\) operations.\n",
        "\n",
        "If \\( C = B^\\top \\) without cloning memory, \\( C \\) and \\( B \\) share the same memory. In this case, \\( AC^\\top \\) is equivalent to \\( AB \\), and there is no difference in speed. However, if \\( C \\) is cloned, additional memory is used, which can slow down the computation.\n",
        "\n",
        "---\n",
        "\n",
        "### 12. Construct a tensor with three axes by stacking \\( A, B, C \\in \\mathbb{R}^{100 \\times 200} \\). What is the dimensionality? Slice out the second coordinate of the third axis to recover \\( B \\). Check that your answer is correct.\n",
        "\n",
        "- Stacking \\( A, B, C \\) along a new axis creates a tensor of shape \\( (3, 100, 200) \\).\n",
        "- To slice out the second coordinate of the third axis (corresponding to \\( B \\)):\n",
        "  ```python\n",
        "  B_recovered = tensor[1, :, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "d2l",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
