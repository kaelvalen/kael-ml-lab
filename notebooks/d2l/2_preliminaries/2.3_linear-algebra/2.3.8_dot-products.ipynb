{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.3.8 Dot Products"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dot Product of Vectors\n",
        "\n",
        "Beyond elementwise operations, sums, and averages, one of the most fundamental operations in linear algebra is the **dot product**.  \n",
        "\n",
        "#### Definition\n",
        "- Given two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, their **dot product** (also called the **inner product**, denoted as $\\mathbf{x}^\\top \\mathbf{y}$ or $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$) is calculated as:  \n",
        "  $\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^d x_i y_i.$$  \n",
        "- It is the sum of the products of the elements at the same positions in the two vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "x = torch.arange(3, dtype=torch.float32)\n",
        "y = torch.ones(3, dtype=torch.float32)\n",
        "x, y, torch.dot(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternative Calculation of the Dot Product\n",
        "\n",
        "The **dot product** of two vectors can also be calculated in an alternative way:  \n",
        "1. Perform an **elementwise multiplication** of the two vectors.  \n",
        "2. Compute the **sum** of the resulting products.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(3.)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.sum(x * y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Applications of Dot Products\n",
        "\n",
        "Dot products are widely used in various contexts:  \n",
        "1. **Weighted Sum**:  \n",
        "   - Given a vector $\\mathbf{x} \\in \\mathbb{R}^n$ (values) and $\\mathbf{w} \\in \\mathbb{R}^n$ (weights), the **weighted sum** of $\\mathbf{x}$ according to $\\mathbf{w}$ is expressed as the dot product:  \n",
        "     $\\mathbf{x}^\\top \\mathbf{w}.$$  \n",
        "\n",
        "2. **Weighted Average**:  \n",
        "   - If the weights $\\mathbf{w}$ are nonnegative and sum to 1 (i.e., $\\sum_{i=1}^n w_i = 1$), the dot product represents a **weighted average**.  \n",
        "\n",
        "3. **Cosine Similarity**:  \n",
        "   - After normalizing two vectors to have unit length, their dot product expresses the **cosine of the angle** between them.  \n",
        "\n",
        "Later, the concept of **length** will be formally introduced to explain normalization."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "d2l",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
