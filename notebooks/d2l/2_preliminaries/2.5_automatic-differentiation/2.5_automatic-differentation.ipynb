{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.5.1 A Simple Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automatic Differentiation\n",
        "\n",
        "## Introduction\n",
        "Recall from Section 2.4 that calculating derivatives is the crucial step in all the optimization algorithms that we will use to train deep networks. While the calculations are straightforward, working them out by hand can be tedious and error-prone, and these issues only grow as our models become more complex.\n",
        "\n",
        "## Autograd Overview\n",
        "Fortunately all modern deep learning frameworks take this work off our plates by offering automatic differentiation (often shortened to autograd). As we pass data through each successive function, the framework builds a computational graph that tracks how each value depends on others. To calculate derivatives, automatic differentiation works backward through this graph applying the chain rule.\n",
        "\n",
        "## Backpropagation\n",
        "The computational algorithm for applying the chain rule in this fashion is called backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Historical Context\n",
        "While autograd libraries have become a hot concern over the past decade, they have a long history. In fact the earliest references to autograd date back over half of a century (Wengert, 1964). The core ideas behind modern backpropagation date to a PhD thesis from 1980 (Speelpenning, 1980) and were further developed in the late 1980s (Griewank, 1989).\n",
        "\n",
        "## Alternative Methods\n",
        "While backpropagation has become the default method for computing gradients, it is not the only option. For instance, the Julia programming language employs forward propagation (Revels et al., 2016).\n",
        "\n",
        "## Next Steps\n",
        "Before exploring methods, let's first master the autograd package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Turkish Summary (Türkçe Özet)\n",
        "Bölüm 2.4'ten hatırlayacağımız gibi, türevleri hesaplamak, derin ağları eğitmek için kullanacağımız tüm optimizasyon algoritmalarında kritik bir adımdır. Hesaplamalar basit olsa da, elle yapılması yorucu ve hata yapmaya açık olabilir, ve bu sorunlar modellerimiz daha karmaşık hale geldikçe artar.\n",
        "\n",
        "Neyse ki, tüm modern derin öğrenme çerçeveleri, otomatik farklılaşma (genellikle autograd olarak kısaltılır) sunarak bu işi bizim üzerimizden alır. Veriyi her bir ardışık fonksiyondan geçirdiğimizde, çerçeve, her bir değerin diğerlere nasıl bağlı olduğunu izleyen bir hesaplamalı grafik oluşturur. Türevleri hesaplamak için, otomatik farklılaşma bu grafik üzerinden zincir kuralını uygulayarak geriye doğru çalışır.\n",
        "\n",
        "Bu şekilde zincir kuralını uygulayan hesaplamalı algoritmaya geri yayılım (backpropagation) denir."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
