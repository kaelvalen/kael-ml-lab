{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.4.3 Partial Derivatives And Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Partial Derivatives and Gradients\n",
        "\n",
        "## Multivariate Functions\n",
        "Thus far, we have been differentiating functions of just one variable. In deep learning, we also need to work with functions of many variables. We briefly introduce notions of the derivative that apply to such multivariate functions.\n",
        "\n",
        "## Partial Derivatives\n",
        "Let y = f(x₁, x₂, ..., xₙ) be a function with n variables. The partial derivative of y with respect to its i-th parameter xᵢ is:\n",
        "\n",
        "∂y/∂xᵢ = lim(h→0) [f(x₁, ..., xᵢ₋₁, xᵢ + h, xᵢ₊₁, ..., xₙ) - f(x₁, ..., xᵢ, ..., xₙ)] / h\n",
        "\n",
        "To calculate ∂y/∂xᵢ, we can treat x₁, ..., xᵢ₋₁, xᵢ₊₁, ..., xₙ as constants and calculate the derivative of y with respect to xᵢ.\n",
        "\n",
        "## Notation Conventions\n",
        "The following notational conventions for partial derivatives are all common and all mean the same thing:\n",
        "\n",
        "∂y/∂xᵢ = ∂f/∂xᵢ = ∂ₓᵢ f = ∂ᵢ f = fₓᵢ = fᵢ = Dᵢ f = Dₓᵢ f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradients\n",
        "We can concatenate partial derivatives of a multivariate function with respect to all its variables to obtain a vector that is called the gradient of the function. Suppose that the input of function f: ℝⁿ → ℝ is an n-dimensional vector x = [x₁, x₂, ..., xₙ]ᵀ and the output is a scalar. The gradient of the function f with respect to x is a vector of n partial derivatives:\n",
        "\n",
        "∇ₓ f(x) = [∂ₓ₁ f(x), ∂ₓ₂ f(x), ..., ∂ₓₙ f(x)]ᵀ\n",
        "\n",
        "When there is no ambiguity, ∇ₓ f(x) is typically replaced by ∇f(x)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Differentiation Rules for Multivariate Functions\n",
        "The following rules come in handy for differentiating multivariate functions:\n",
        "\n",
        "- For all A ∈ ℝᵐˣⁿ we have ∇ₓ(Ax) = Aᵀ and ∇ₓ(xᵀA) = A\n",
        "- For square matrices A ∈ ℝⁿˣⁿ we have that ∇ₓ(xᵀAx) = (A + Aᵀ)x and in particular ∇ₓ‖x‖² = ∇ₓ(xᵀx) = 2x\n",
        "- Similarly, for any matrix X, we have ∇ₓ‖X‖²_F = 2X"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
